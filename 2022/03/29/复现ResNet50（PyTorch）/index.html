<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="复现ResNet50（PyTorch）"><meta name="keywords" content="深度学习,ResNet"><meta name="author" content="Met Guo"><meta name="copyright" content="Met Guo"><title>复现ResNet50（PyTorch） | Gmet's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LEZCUKXOU8","apiKey":"43c226e390cd280e256d1d7cc0e6cce4","indexName":"blogs","hits":{"per_page":10,"input_placeholder":"Search for Posts","hits_empty":"û���ҵ�����Ҫ�Ľ��: ${query}","hits_stats":"�ҵ�${hits}���������ʱ${time}���룩"},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.2'
} </script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Gmet's Blog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%8D%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">复现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E5%8C%85"><span class="toc-number">2.1.</span> <span class="toc-text">导包</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet50-%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">ResNet50 生成函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet%E7%B1%BB"><span class="toc-number">2.3.</span> <span class="toc-text">ResNet类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#init"><span class="toc-number">2.3.1.</span> <span class="toc-text">init</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#forward"><span class="toc-number">2.3.2.</span> <span class="toc-text">forward</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bottleneck"><span class="toc-number">2.4.</span> <span class="toc-text">Bottleneck</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E%E8%BE%93%E5%85%A5%E5%88%B0layers"><span class="toc-number">3.</span> <span class="toc-text">从输入到layers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#layer1-Bottleneck1"><span class="toc-number">3.1.</span> <span class="toc-text">layer1&#x2F;Bottleneck1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer1-Bottleneck2"><span class="toc-number">3.2.</span> <span class="toc-text">layer1&#x2F;Bottleneck2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer1-Bottleneck3"><span class="toc-number">3.3.</span> <span class="toc-text">layer1&#x2F;Bottleneck3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer2-Bottleneck1"><span class="toc-number">3.4.</span> <span class="toc-text">layer2&#x2F;Bottleneck1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer2-Bottleneck234"><span class="toc-number">3.5.</span> <span class="toc-text">layer2&#x2F;Bottleneck234</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#layer3-4"><span class="toc-number">3.6.</span> <span class="toc-text">layer3&#x2F;4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E5%8D%B0%E5%AE%8C%E6%95%B4%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">3.7.</span> <span class="toc-text">打印完整的网络结构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">4.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Met Guo</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">52</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">44</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">38</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://images7.alphacoders.com/550/thumb-1920-550739.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Gmet's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/slides">Slides</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">复现ResNet50（PyTorch）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-03-29</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ResNet/">ResNet</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 19 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>本篇博客介绍了 ResNet50 网络 PyTorch 复现（复现代码为 PyTorch 源码）</p>
</blockquote>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>Residual Net：残差网络。</p>
<p>将靠前若干层的某一层数据输出直接跳过多层引入到后面数据层的输入部分。意味着后面的特征层的内容会有一部分由其前面的某一层线性贡献。</p>
<p><img src="image-20220329215711900.png" alt="残差块"></p>
<p>深度残差网络的设计是为了克服由于网络深度加深而产生的学习效率变低与准确率无法有效提升的问题。</p>
<p>下面这张图来自参考文献[1]，给出了ResNet18、ResNet34、ResNet50、ResNet101、ResNet152的块结构。</p>
<p><img src="image-20220329220108463.png" alt="ResNet-XX 网络结构"></p>
<p>根据该图，我们可以画出ResNet50的网络结构，</p>
<p><img src="image-20220329220245960.png" alt="ResNet-50 网络结构"></p>
<p>由上述网络结构可以看到，ResNet包含两种Block：分别为<strong>Conv Block</strong>和<strong>Identity Block</strong>。</p>
<ul>
<li><strong>Conv Block</strong>输入和输出的维度（通道数和size）是不一样的，所以相同的Conv Block不能连续串联，它的作用是改变网络的维度；</li>
<li><strong>Identity Block</strong>输入维度和输出维度（通道数和size）相同，可以串联，用于加深网络的。</li>
</ul>
<p>下面是两种块的结构：</p>
<p><img src="image-20220329221454060.png" alt="Conv Block"></p>
<p><img src="image-20220329221531582.png" alt="Identity Block"></p>
<p>这两种块的区别在于残差边是否卷积。如果经过$1*1$卷积。经过卷积的是Conv Block，直连的是Identity Block。</p>
<p>稍后用一个类<code>Bottleneck</code>实现这两种块。</p>
<p>下面参考PyTorch框架源码，对 ResNet50  一步步复现。</p>
<h1 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h1><h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br></pre></td></tr></table></figure>
<h2 id="ResNet50-生成函数"><a href="#ResNet50-生成函数" class="headerlink" title="ResNet50 生成函数"></a>ResNet50 生成函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">pretrained = <span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Constructs a ResNet-50 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained:(bool): If True, return a model pretrained on ImageNet</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        <span class="comment"># model.load_state_dict(model_zoo.load_url(model_urls[&#x27;resnet50&#x27;]))</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">net = resnet50()</span><br></pre></td></tr></table></figure>
<p>通过 <code>model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)</code> 构造网络结构，主要分成两个部分即 <code>Bottleneck</code>和 <code>[3,4,6,3]</code> 由这两个参数共同决定了ResNet50的网络结构 ，当<code>pretrained</code>为<code>True</code>时，model加载ImageNet中预训练的参数。（这块本篇文章先不考虑）</p>
<p><code>[3,4,6,3]</code>对应于上图中ResNet50中 conv2_x中有三个（$1<em>1, 64$，$3</em>3, 64$，$1<em>1, 256$）卷积层的堆叠 ，同理conv3_x中有4个（$1</em>1, 128$，$3<em>3, 128$，$1</em>1, 512$）卷积层的堆叠，ResNet50将卷积层分为4个大层，<code>[3,4,6,3]</code>代表每一个大层中$1<em>1$，$3</em>3$，$1<em>1$卷积层组合的重复次数总共1（第一个卷积层）+1（第一个池化层）+（3+4+6+3）</em>3 = 50层。</p>
<p>这里<code>Bottleneck</code>类，就是一个基础块。对应上图（$1<em>1$，$3</em>3$，$1*1$）的三个卷积层组合。</p>
<h2 id="ResNet类"><a href="#ResNet类" class="headerlink" title="ResNet类"></a>ResNet类</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, planes, blocks, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,</span><br><span class="line">                kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(planes * block.expansion)</span><br><span class="line">            )</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, downsample))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks): <span class="comment"># for (blocks - 1)</span></span><br><span class="line">            layers.append(block(self.inplanes, planes))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layers, num_classes = <span class="number">1000</span></span>):</span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__() <span class="comment"># ?</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">7</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#因为最后feature map在输入为224时，经过layer4之后大小为7*7，此时经过nn.AvgPool2d(7, stride=1)大小变为1*1，再经过全连接层时，self.fc = nn.Linear(512 * block.expansion, num_classes) 前者是输出的所有channel数目，实际应该为channel*1*1，后者为分类数</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line">		<span class="comment"># ?</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">       self.avgpool = nn.AvgPool2d(<span class="number">7</span>, stride=<span class="number">1</span>)</span><br><span class="line">  x = self.avgpool(x)  </span><br><span class="line"></span><br><span class="line">  x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)     </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 首先对输入进行7*7的卷积</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="comment"># 然后对x进行3*3的最大池化</span></span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">		<span class="comment"># 接着进入四个layer/stage</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">		<span class="comment">#最后平均池化</span></span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># 将数据拉伸成batchsize * channel * 1 * 1 ?</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 如果输入大小不为224  那么相应的可以修改AvgPool2d 或者在全连接层第一个参数中乘上最终的width 和height</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>ResNet 是由<code>__init__</code>和<code>forward</code>构成，为了方便分析这里首先分析<code>__init__</code>函数。</p>
<h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><p>在<code>__init__</code>中，最重要的是<code>_make_layer</code>函数，以<code>layer1</code>为例，block为Bottleneck，planes=64（即channel数目）blocks=3 （<code>[3,4,6,3]</code>分别代表每一层的blocks数目）这里要注意<code>layer1</code>的stride为1，其他layer的stride为2。</p>
<p>对于<code>layer1</code>而言，<code>inplanes=64</code>， <code>planes=64</code>， <code>block.expansion=4</code>，因此需要经过downsample才能够使得残差和经过该层的feature map能够相加，downsample即为右路部分。（可以看<code>Bottleneck</code>的实现）</p>
<h3 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h3><p>见上面的代码注释。</p>
<h2 id="Bottleneck"><a href="#Bottleneck" class="headerlink" title="Bottleneck"></a>Bottleneck</h2><p>接着我们看一下Bottleneck的实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Block的各个plane值：</span></span><br><span class="line"><span class="string">        inplanes：输出block的之前的通道数</span></span><br><span class="line"><span class="string">        planes：在block中间处理的时候的通道数（这个值是输出维度的1/4）</span></span><br><span class="line"><span class="string">        midplane*self.expansion：输出的维度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment">#每个stage中维度拓展的倍数</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1 * 1卷积核不改变feature map的大小，3 * 3卷积核padding=1&amp;&amp;stride=1也不改变输入feature map的大小，因此经过一个Bottleneck组成的卷积层组操作后feature map大小不会改变</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride = <span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace= <span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>其中当<code>downsample</code>为None时，残差边直连，此时为Identify Block，当<code>downsample</code>不为None时，残差边进行卷积再相加，此时为Conv Block。</p>
<blockquote>
<p>为什么要先卷积再相加呢？</p>
<p>因为feature map的大小不变 但是在经过Bottleneck 之后channel变成了原来的四倍，因此想要和原始的feature map相加需要将原始的feature map也变为原来的四倍 ，downsample作用是residual+当前feature map时将维度统一。</p>
</blockquote>
<h1 id="从输入到layers"><a href="#从输入到layers" class="headerlink" title="从输入到layers"></a>从输入到layers</h1><p>首先输入(3, 224, 224)，即三个通道，224<em>224像素的输入，经过一个输出通道数为64的7\</em>7卷积层，一个3*3池化，得到(64, 56, 56)。然后将其输入到layer1、2、3、4。</p>
<blockquote>
<p>解释一下为什么是(64, 56, 56)</p>
<p>因为ResNet接受的图像大小为224 * 224 经过第一层卷积层<code>self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)</code></p>
<script type="math/tex; mode=display">
floor((224-7+3*2)/2)+1=112</script><p>经过第一层池化之后，<code>self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)</code></p>
<script type="math/tex; mode=display">
floor((112+2*1-3)/2)+1=56</script><p>因此在输入到Bottleneck之前得到一个64(channel), 56(height)，56(weight)大小的feature map。</p>
</blockquote>
<h2 id="layer1-Bottleneck1"><a href="#layer1-Bottleneck1" class="headerlink" title="layer1/Bottleneck1"></a>layer1/Bottleneck1</h2><p>输入 ：[batch_size,64,56,56]</p>
<p>调用<code>_make_layer(block, 64, layers[0])</code>构造了layer1，此时<code>self.inplanes</code>为64，<code>planes * block.expansion</code>=64 <em> 4，不相等（之所以要二者相等，是因为在<code>Bottleneck</code>主体分支最后一个卷积层会将<code>channel</code>变为`planes</em>block.expansion<code>，如果</code>inplanes<code>（实际就是输入的channel）与之不相等则不可相加
因此构造右路</code>downsample` （1*1卷积核的卷积层扩展channel+BN层）</p>
<p><img src="20181108104951863.png" alt="主体分支"></p>
<p><img src="20181108105037539.png" alt="downsample分支"></p>
<p>更新 inplanes=64*4=256</p>
<h2 id="layer1-Bottleneck2"><a href="#layer1-Bottleneck2" class="headerlink" title="layer1/Bottleneck2"></a>layer1/Bottleneck2</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv3 = nn.Conv2d(<span class="number">64</span>,<span class="number">256</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">self.downsample = <span class="literal">None</span></span><br><span class="line">self.stride = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="layer1-Bottleneck3"><a href="#layer1-Bottleneck3" class="headerlink" title="layer1/Bottleneck3"></a>layer1/Bottleneck3</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=stride,padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv3 = nn.Conv2d(<span class="number">64</span>,<span class="number">256</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">self.downsample = <span class="literal">None</span></span><br><span class="line">self.stride = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="layer2-Bottleneck1"><a href="#layer2-Bottleneck1" class="headerlink" title="layer2/Bottleneck1"></a>layer2/Bottleneck1</h2><p>此时<code>stride</code>=2，<code>self.inplanes</code>=256， <code>planes * block.expansion</code>=128*4，需要生成downsample层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">downsample = nn.Sequential(</span><br><span class="line">	nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>,kernel_size=<span class="number">1</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">	nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>生成第一个Bottleneck的主干</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line"><span class="comment"># 此时feature map 大小由56变成28</span></span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">self.conv3 = nn.Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">self.downsample = downsample</span><br><span class="line">self.stride = stride</span><br></pre></td></tr></table></figure>
<p>更新inplanes=512</p>
<h2 id="layer2-Bottleneck234"><a href="#layer2-Bottleneck234" class="headerlink" title="layer2/Bottleneck234"></a>layer2/Bottleneck234</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">self.conv3 = nn.Conv2d(<span class="number">128</span>,<span class="number">256</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">self.bn3 = nn.BatchNorm2d(planes * self.expansion)</span><br><span class="line">self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">self.downsample = <span class="literal">None</span></span><br><span class="line">self.stride = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="layer3-4"><a href="#layer3-4" class="headerlink" title="layer3/4"></a>layer3/4</h2><p>对于layer3和layer4同理。</p>
<h2 id="打印完整的网络结构"><a href="#打印完整的网络结构" class="headerlink" title="打印完整的网络结构"></a>打印完整的网络结构</h2><p>最后，我们打印完整的网络结构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (4): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (5): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)</span><br><span class="line">  (fc): Linear(in_features=2048, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li>He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a940902940902/article/details/83858694">ResNet结构分析</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1154y1S7WC">史上最详细ResNet50复现解析（面向小白）</a></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Met Guo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://guoyujian.github.io/2022/03/29/%E5%A4%8D%E7%8E%B0ResNet50%EF%BC%88PyTorch%EF%BC%89/">https://guoyujian.github.io/2022/03/29/%E5%A4%8D%E7%8E%B0ResNet50%EF%BC%88PyTorch%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://guoyujian.github.io">Gmet's Blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/ResNet/">ResNet</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/04/04/%E5%9F%BA%E4%BA%8EResNet%E7%9A%84%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/"><i class="fa fa-chevron-left">  </i><span>基于ResNet的树叶分类任务</span></a></div><div class="next-post pull-right"><a href="/2022/03/09/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91BFS-%E7%AE%97%E6%B3%95%E8%A7%A3%E9%A2%98%E5%A5%97%E8%B7%AF%E6%A1%86%E6%9E%B6/"><span>【笔记】BFS 算法解题套路框架</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://images7.alphacoders.com/550/thumb-1920-550739.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By Met Guo</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>