<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Visual Attention Network"><meta name="keywords" content="深度学习,计算机视觉"><meta name="author" content="Met Guo"><meta name="copyright" content="Met Guo"><title>Visual Attention Network | Gmet's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LEZCUKXOU8","apiKey":"43c226e390cd280e256d1d7cc0e6cce4","indexName":"blogs","hits":{"per_page":10,"input_placeholder":"Search for Posts","hits_empty":"û���ҵ�����Ҫ�Ľ��: ${query}","hits_stats":"�ҵ�${hits}���������ʱ${time}���룩"},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.2'
} </script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Gmet's Blog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.</span> <span class="toc-text">自注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E7%9B%B8%E5%85%B3%E6%80%A7%E5%92%8C%E9%80%9A%E9%81%93%E7%9B%B8%E5%85%B3%E6%80%A71"><span class="toc-number">1.2.</span> <span class="toc-text">空间相关性和通道相关性1</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E6%96%872"><span class="toc-number">2.</span> <span class="toc-text">正文2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-number">2.1.</span> <span class="toc-text">动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LCK"><span class="toc-number">2.2.1.</span> <span class="toc-text">LCK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAN"><span class="toc-number">2.2.2.</span> <span class="toc-text">VAN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.1.</span> <span class="toc-text">图像分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">2.3.2.</span> <span class="toc-text">目标检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">2.3.3.</span> <span class="toc-text">语义分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.3.4.</span> <span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">2.3.5.</span> <span class="toc-text">可视化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Refs"><span class="toc-number">3.</span> <span class="toc-text">Refs</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Met Guo</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">87</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">70</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">47</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://images7.alphacoders.com/550/thumb-1920-550739.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Gmet's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/slides">Slides</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Visual Attention Network</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-02-04</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 10 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>本文是论文《Visual Attention Network》的学习笔记</p>
<p>我在该模型上执行一个分类任务，发现该模型的效果要优于我实验的其他模型（包括resnet50、densenet121、efficientNet-b0、swin-tiny）</p>
<p>所以在此记录一下，笔记大多是抄的，出处在Refs上表明，如有侵权请联系我。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.09741">论文地址</a>   <a target="_blank" rel="noopener" href="https://github.com/Visual-Attention-Network">代码地址</a></p>
</blockquote>
<p>‍</p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/265108616">Attention注意力机制与self-attention自注意力机制</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/100902">什么是自注意力机制？</a></p>
<h2 id="空间相关性和通道相关性1"><a href="#空间相关性和通道相关性1" class="headerlink" title="空间相关性和通道相关性1"></a>空间相关性和通道相关性<sup><a href="#fn_1" id="reffn_1">1</a></sup></h2><p>从维度的角度看，卷积核可以看成是一个空间维(宽和高)和通道维的组合，而<strong>卷积操作则可以视为空间相关性和通道相关性的联合映射</strong>。从inception的1x1卷积来看，<strong>卷积中的空间相关性和通道相关性是可以解耦的，将它们分开进行映射，可能会达到更好的效果。</strong></p>
<p>深度可分离卷积是在1x1卷积基础上的一种创新。主要包括两个部分：深度卷积和1x1卷积。深度卷积的目的在于对输入的每一个通道都单独使用一个卷积核对其进行卷积，也就是通道分离后再组合。1x1卷积的目的则在于加强深度。下面以一个例子来看一下深度可分离卷积。</p>
<p>假设我们用128个$3 \times3 \times3$的滤波器对一个 $7 \times7 \times3$的输入进行卷积，可得到$5 \times5 \times128$的输出,其计算量为$5 \times5 \times128 \times3 \times3 \times3=86400$。如下图所示：</p>
<p><img src="image-20230204110644-zz7yeyj.png" alt="image">​</p>
<p>现在看如何使用深度可分离卷积来实现同样的结果。深度可分离卷积的第一步是深度卷积。这里的深度卷积，就是分别用3个$3 \times3 \times1$的滤波器对输入的3个通道分别做卷积，也就是说要做3次卷积，每次卷积都有一个$5 \times5 \times1$的输出，组合在一起便是$5 \times5 \times3$的输出。现在为了拓展深度达到128，我们需要执行深度可分离卷积的第二步：1x1卷积。现在我们用128个$1 \times1 \times3$的滤波器对$5 \times5 \times3$进行卷积，就可以得到$5 \times5 \times128$的输出。完整过程如下图所示：</p>
<p><img src="image-20230204111234-3ia5cu3.png" alt="image">​</p>
<p>那么我们来看一下深度可分离卷积的计算量如何。第一步深度卷积的计算量：$5 \times5 \times1 \times3 \times3 \times1 \times3=675$。第二步1x1卷积的计算量：$5 \times5 \times128 \times1 \times1 \times3=9600$，合计计算量为10275次。可见，相同的卷积计算输出，深度可分离卷积要比常规卷积节省12倍的计算成本。</p>
<blockquote>
<p>典型的应用深度可分离卷积的网络模型包括xception和mobilenet等。本质上而言，xception就是应用了深度可分离卷积的inception网络。</p>
</blockquote>
<h1 id="正文2"><a href="#正文2" class="headerlink" title="正文2"></a>正文<sup><a href="#fn_2" id="reffn_2">2</a></sup></h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>图像的二维性质给在计算机视觉中应用自注意力带来了三个挑战：</p>
<ol>
<li>将图像处理为一维序列，忽略了其二维结构。</li>
<li>二次复杂度对于高分辨率的图像来说太贵了。</li>
<li>它只捕捉了空间适应性，而忽略了通道适应性</li>
</ol>
<p>在本文中，作者提出了一种新的大核注意(LKA)模块，以使自注意的自适应和长程相关，同时避免了上述问题。作者进一步介绍了一种基于LKA的新的神经网络，即视觉注意网络(VAN)。VAN虽然非常简单和高效，但在包括图像分类、目标检测、语义分割、实例分割等广泛的实验中，它以很大的优势优于最先进的transfomer和卷积神经网络。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="LCK"><a href="#LCK" class="headerlink" title="LCK"></a>LCK</h3><p>卷积神经网络(CNNs)利用局部上下文信息和平移不变性，大大提高了神经网络的效率。自AlexNet以来，cnn迅速成为计算机视觉的主要主流框架。为了进一步提高效率，研究人员投入了大量的精力，使cnn成为更深的和更轻的。作者的工作与MobileNet有相似之处，MobileNet将标准卷积解耦为两部分，即深度卷积和逐点卷积(也就是1×1Conv)。作者的方法将卷积分解为三个部分：深度卷积、深度空洞卷积和逐点卷积。得益于这种分解，作者的方法更适合于有效地分解大的核卷积。作者还在该方法中引入了注意机制来获得自适应特性。</p>
<p><img src="v2-467f001629900492069a79a14d2dc757_720w.webp" alt="img"></p>
<p>彩色网格表示卷积核的位置，黄色网格表示中心点。从图中可以看出，13×13卷积分解为5×5深度卷积，5×5深度空洞卷积，膨胀速率3和1×1卷积</p>
<p><img src="v2-d22b7728ed41dd3491530ad8a94fbe19_720w.webp" alt="img"></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">class AttentionModule(nn.Module):</span><br><span class="line">    def __init__(self, dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)#深度卷积</span><br><span class="line">        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)#深度空洞卷积</span><br><span class="line">        self.conv1 = nn.Conv2d(dim, dim, 1)#逐点卷积</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        u = x.clone()        </span><br><span class="line">        attn = self.conv0(x)</span><br><span class="line">        attn = self.conv_spatial(attn)</span><br><span class="line">        attn = self.conv1(attn)</span><br><span class="line"></span><br><span class="line">        return u * attn   #注意力操作</span><br><span class="line">     </span><br><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, d_model):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.proj_1 = nn.Conv2d(d_model, d_model, 1)</span><br><span class="line">        self.activation = nn.GELU()</span><br><span class="line">        self.spatial_gating_unit = AttentionModule(d_model)  #注意力操作</span><br><span class="line">        self.proj_2 = nn.Conv2d(d_model, d_model, 1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        shorcut = x.clone()</span><br><span class="line">        x = self.proj_1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.spatial_gating_unit(x)  #注意力操作</span><br><span class="line">        x = self.proj_2(x)</span><br><span class="line">        x = x + shorcut   #残差连接</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">class Block(nn.Module):</span><br><span class="line">    def __init__(self, dim, mlp_ratio=4., drop=0.,drop_path=0., act_layer=nn.GELU):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.norm1 = nn.BatchNorm2d(dim)</span><br><span class="line">        self.attn = SpatialAttention(dim)</span><br><span class="line">        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.norm2 = nn.BatchNorm2d(dim)</span><br><span class="line">        mlp_hidden_dim = int(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line">        layer_scale_init_value = 1e-2            </span><br><span class="line">        self.layer_scale_1 = nn.Parameter(</span><br><span class="line">            layer_scale_init_value * torch.ones((dim)), requires_grad=True)</span><br><span class="line">        self.layer_scale_2 = nn.Parameter(</span><br><span class="line">            layer_scale_init_value * torch.ones((dim)), requires_grad=True)</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    def _init_weights(self, m):</span><br><span class="line">        if isinstance(m, nn.Linear):</span><br><span class="line">            trunc_normal_(m.weight, std=.02)</span><br><span class="line">            if isinstance(m, nn.Linear) and m.bias is not None:</span><br><span class="line">                nn.init.constant_(m.bias, 0)</span><br><span class="line">        elif isinstance(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, 0)</span><br><span class="line">            nn.init.constant_(m.weight, 1.0)</span><br><span class="line">        elif isinstance(m, nn.Conv2d):</span><br><span class="line">            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels</span><br><span class="line">            fan_out //= m.groups</span><br><span class="line">            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))</span><br><span class="line">            if m.bias is not None:</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.attn(self.norm1(x)))#drop_path分支中，每个batch有概率使样本在self.attn或者mlp不会”执行“，会以0直接传递。</span><br><span class="line">        x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))</span><br><span class="line">        return </span><br></pre></td></tr></table></figure>
<p>作者提出对大核卷积操作进行分解来捕获长程关系。大核卷积可分为三个部分：空间局部卷积（深度卷积）、空间远程卷积（深度空洞卷积）和通道卷积（1×1卷积）。所以，可以将K×K卷积分解为K/d×K/d的深度空洞卷积，(2d−1)×(2d−1)的深度卷积和1×1卷积。通过上述分解，可以用轻微的计算代价和参数来捕获长程关系。在得到长程关系后，可以估计一个点的重要性并生成注意力图。</p>
<p><img src="v2-f2a083b057051de4ae3a87fd77138a8e_720w.webp" alt="img"></p>
<p><img src="v2-6109ccc0466efee70bf7ac2324513a09_720w.webp" alt="img"></p>
<p>作者提出的LKA结合了卷积和自注意力的优点。它考虑了局部上下文信息、大的感受野和动态过程。此外，LKA不仅实现了空间维度的自适应性，而且还实现了通道维度的自适应性。值得注意的是，在深度神经网络中，不同的通道往往代表不同的对象，而通道维度的适应性对视觉任务也很重要。</p>
<h3 id="VAN"><a href="#VAN" class="headerlink" title="VAN"></a>VAN</h3><p>VAN具有简单的层次结构，即输出空间分辨率降低的四个阶段序列，分别为H/4×W/4、H/8×W/8、H/16×W/16和H/32×W/32。H和W是输入图像的高度和宽度。随着分辨率的降低，输出通道的数量也在不断增加。输出通道Ci的变化如下表所示。 首先对输入值进行下采样，并使用步幅数来控制下采样率。下采样后，一个stage中的所有层保持相同的输出大小，即空间分辨率和通道数量。然后，批量归一化、GELU激活函数、大核注意和卷积前馈网络依次堆叠，提取特征。最后，在每个阶段结束时应用一个层归一化。根据参数和计算成本，设计了 VAN-Tiny, VAN-Small, VAN-Base and VAN-Large四种结构。</p>
<p><img src="v2-9c29743630133af223ffb0b9d3c592f0_720w.webp" alt="img"></p>
<p><img src="v2-f3d5e05ccc5b5d1691c26fc3d8daafcd_720w.webp" alt="img"></p>
<p>对21×21卷积的不同方式参数的比较。X，Y和our分别提供了标准卷积，mobilenet]和van的分解。输入和输出具有相同大小的H×W×C</p>
<p><img src="v2-f39da3202b78c90cd02669ff5039e8a9_720w.webp" alt="img"></p>
<p>默认情况下，本文的LKA采用5×5深度卷积，7×7深度卷积与膨胀率为3的空洞卷积和1×1卷积来近似21×21卷积。在这种设置下，VAN可以有效地获取局部信息和长程联系。作者分别使用7×7和3×3步幅卷积进行4×和2×的降采样。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在ImageNet-1K图像分类数据集、COCO目标检测数据集和ADE20K语义分割数据集上进行了定量实验。此外，作者通过在ImageNet验证集上使用Grad-CAM来可视化类激活映射(CAM)。</p>
<h3 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h3><p><img src="https://pic2.zhimg.com/80/v2-2fefbe89e8e7da997c3683528fbb2535_720w.webp" alt="img"></p>
<p>VAN与其他mlp、cnn和ViTs的比较。VAN优于其他参数相似，计算成本相似的cnn(ResNet[29]，ResNeXt[90]，ConvNeXt[53]等)，ViTs(DeiT[74]、PVT[83]、Swin-Transformer[52]等)和MLPs(MLP-Mixer[72]，ResMLP[73]，gMLP[46]等)。作者在每个类别中选择了一个具有代表性的网络进行讨论。ConvNeXt[53]是一种特殊的CNN，它吸收了vit的一些优势，如大的感受野（7×7卷积）和先进的训练策略(300个epoch、数据增强等)。VAN和ConvNeXt[53]相比，VAN-base比CoNvNeXt-t多出0.7%(82.8%vs.82.1%)，因为VAN具有更大的感受域和自适应能力。Swin-Transformer是一种著名的ViT变体，采用局部注意力和移动窗口的方式。由于VAN对二维结构信息非常友好，具有较大的感受野，并在通道维度上实现了自适应性，VAN-Base超过Swin-T1.5%(82.8%vs.81.3%)。对于MLPs，选择gMLP[46]。VAN-Base超过gMLP-S[46]3.2%(82.8%vs.79.6%)。也可以看出，在小型模型上面VAN的表现更加出色。</p>
<h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p><img src="v2-c04b7aef7af9212da50e947198ea0179_720w.webp" alt="img"></p>
<p><img src="v2-ac6fd9136c34fa6ebfce940f20b6a318_720w.webp" alt="img"></p>
<p><img src="v2-f0a76550ee33afe78ff4d4458f544c25_720w.webp" alt="img"></p>
<h3 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h3><p><img src="v2-5675289d453da40b8f33f28c1966aa44_720w.webp" alt="img"></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p><img src="v2-1f9970a60d196d053e6433e7bbcca814_720w.webp" alt="img"></p>
<p>DW-D-Conv提供了深度空洞卷积，这在捕获LKA中的长程依赖性中发挥了作用。</p>
<p>DW-Conv可以利用图像的局部上下文信息。</p>
<p>注意力机制的引入可以看作是使网络实现了自适应特性。受益于此，VAN-Tiny实现了约1.1%（74.3%对75.4%）的改善。</p>
<p>1×1Conv捕获了通道维度中的关系。结合注意机制，引入了通道维度的自适应性。提高了0.8%(74.1%vs.75.4%)，证明了通道维度自适应性的必要性。</p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p><img src="v2-84a72be357178fade018043de0213190_720w.webp" alt="img"></p>
<p>可视化结果。所有的图像都来自于ImageNet验证集中的不同类别。CAM采用VAN-Base模型和Grad-CAM产生。左：原始图像，右：类激活图</p>
<p>结果显示，VAN-Base可以清晰地聚焦于目标对象。因此，可视化直观地证明了VAN的有效性。</p>
<p>‍</p>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><blockquote id="fn_1">
<sup>1</sup>. <a target="_blank" rel="noopener" href="https://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97%28%E4%BA%8C%29%20%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%BD%A2%E5%BC%8F/">深度学习知识系列(二) 各种卷积形式</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2 ">
<sup>2 </sup>. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/474526444">【ARXIV2202】Visual Attention Network</a><a href="#reffn_2 " title="Jump back to footnote [2 ] in the text."> &#8617;</a>
</blockquote>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Met Guo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://guoyujian.github.io/2023/02/04/Visual-Attention-Network/">https://guoyujian.github.io/2023/02/04/Visual-Attention-Network/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://guoyujian.github.io">Gmet's Blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/02/25/%E8%BD%AF%E4%BB%B6%E8%A1%8C%E4%B8%9A%E5%B0%B1%E4%B8%9A%E6%96%B9%E5%90%91%E8%B0%83%E7%A0%94/"><i class="fa fa-chevron-left">  </i><span>软件行业就业方向调研</span></a></div><div class="next-post pull-right"><a href="/2022/12/04/Spring-AOP-Demo/"><span>Spring AOP Demo</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://images7.alphacoders.com/550/thumb-1920-550739.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2024 By Met Guo</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>